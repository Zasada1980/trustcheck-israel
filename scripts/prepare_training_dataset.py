"""
–°–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ TrustCheck AI
–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞ + –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å—Å—ã–ª–∫–∞–º –≤ –µ–¥–∏–Ω—ã–π JSON –¥–∞—Ç–∞—Å–µ—Ç
"""

import os
import json
import re
from pathlib import Path
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
PROJECT_ROOT = Path("E:/SBF")
OUTPUT_DIR = Path("E:/LLaMA-Factory/data")
OUTPUT_FILE = OUTPUT_DIR / "trustcheck_knowledge_base.json"

# –†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
ALLOWED_EXTENSIONS = [
    '.md', '.txt', '.sql', '.ts', '.tsx', '.js', '.jsx', 
    '.py', '.yml', '.yaml', '.json', '.env.example'
]

# –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
EXCLUDED_DIRS = [
    'node_modules', '.git', '.next', 'dist', 'build', 
    '__pycache__', '.venv', 'venv', 'archive'
]

def extract_urls_from_text(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á—å –≤—Å–µ URL –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\])]+'
    return re.findall(url_pattern, text)

def fetch_webpage_content(url: str, timeout: int = 10) -> str:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –£–¥–∞–ª–∏—Ç—å —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        for script in soup(["script", "style"]):
            script.decompose()
        
        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator='\n', strip=True)
        
        # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return '\n'.join(lines)
    
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
        return ""

def collect_project_files() -> List[Dict]:
    """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    print("üìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞...")
    
    files_data = []
    file_count = 0
    
    for root, dirs, files in os.walk(PROJECT_ROOT):
        # –ò—Å–∫–ª—é—á–∏—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
        
        for file in files:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            if not any(file.endswith(ext) for ext in ALLOWED_EXTENSIONS):
                continue
            
            file_path = Path(root) / file
            relative_path = file_path.relative_to(PROJECT_ROOT)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                files_data.append({
                    "file_path": str(relative_path),
                    "content": content,
                    "type": "project_file"
                })
                
                file_count += 1
                if file_count % 50 == 0:
                    print(f"  –°–æ–±—Ä–∞–Ω–æ: {file_count} —Ñ–∞–π–ª–æ–≤...")
            
            except Exception as e:
                print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {relative_path}: {e}")
    
    print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {file_count}")
    return files_data

def extract_and_fetch_urls(files_data: List[Dict]) -> List[Dict]:
    """–ò–∑–≤–ª–µ—á—å URL –∏–∑ —Ñ–∞–π–ª–æ–≤ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Ö –∫–æ–Ω—Ç–µ–Ω—Ç"""
    print("\nüåê –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫...")
    
    all_urls = set()
    
    # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL
    for file_data in files_data:
        urls = extract_urls_from_text(file_data['content'])
        all_urls.update(urls)
    
    print(f"  –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(all_urls)}")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è - —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    important_domains = [
        'data.gov.il',
        'ica.justice.gov.il',
        'taxes.gov.il',
        'court.gov.il',
        'boi.org.il',
        'docs.python.org',
        'nextjs.org',
        'postgresql.org',
        'github.com/Zasada1980'
    ]
    
    filtered_urls = [
        url for url in all_urls 
        if any(domain in url for domain in important_domains)
    ]
    
    print(f"  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –≤–∞–∂–Ω—ã—Ö URL: {len(filtered_urls)}")
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
    url_data = []
    for i, url in enumerate(filtered_urls, 1):
        print(f"  [{i}/{len(filtered_urls)}] –ó–∞–≥—Ä—É–∑–∫–∞: {url[:60]}...")
        
        content = fetch_webpage_content(url)
        if content:
            url_data.append({
                "url": url,
                "content": content,
                "type": "external_webpage"
            })
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(url_data)}")
    return url_data

def create_training_dataset(files_data: List[Dict], url_data: List[Dict]) -> List[Dict]:
    """–°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ LLaMA Factory (Alpaca)"""
    print("\nüìù –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    dataset = []
    
    # 1. –§–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞
    for file_data in files_data:
        instruction = f"–û–±—ä—è—Å–Ω–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ {file_data['file_path']} –≤ –ø—Ä–æ–µ–∫—Ç–µ TrustCheck Israel"
        
        dataset.append({
            "instruction": instruction,
            "input": "",
            "output": file_data['content'][:4000],  # –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
            "system": "–¢—ã - AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–µ–∫—Ç–∞ TrustCheck Israel. –ó–Ω–∞–µ—à—å –≤—Å—é –∫–æ–¥–æ–≤—É—é –±–∞–∑—É –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é."
        })
    
    # 2. –í–Ω–µ—à–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    for url_data_item in url_data:
        instruction = f"–ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ —Å—Å—ã–ª–∫–µ {url_data_item['url']}?"
        
        dataset.append({
            "instruction": instruction,
            "input": "",
            "output": url_data_item['content'][:4000],
            "system": "–¢—ã - AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–µ–∫—Ç–∞ TrustCheck Israel —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –≤–Ω–µ—à–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–∞–Ω–Ω—ã—Ö."
        })
    
    # 3. FAQ –ø–æ –ø—Ä–æ–µ–∫—Ç—É
    faq_data = [
        {
            "instruction": "–ß—Ç–æ —Ç–∞–∫–æ–µ TrustCheck Israel?",
            "output": "TrustCheck Israel - —ç—Ç–æ B2C –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏–∑—Ä–∞–∏–ª—å—Å–∫–∏—Ö –±–∏–∑–Ω–µ—Å–æ–≤. –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è: —Ä–æ–¥–∏—Ç–µ–ª–∏, –ø—Ä–æ–≤–µ—Ä—è—é—â–∏–µ —á–∞—Å—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å—ã (–¥–µ—Ç—Å–∫–∏–µ —Å–∞–¥—ã, —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä—ã) –ø–µ—Ä–µ–¥ –æ–ø–ª–∞—Ç–æ–π. –°—Ç–µ–∫: Next.js 14 + PostgreSQL + Google Gemini AI + Docker."
        },
        {
            "instruction": "–ö–∞–∫–∏–µ —Ç–∏–ø—ã –±–∏–∑–Ω–µ—Å–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç TrustCheck?",
            "output": "TrustCheck –ø—Ä–æ–≤–µ—Ä—è–µ—Ç 3 —Ç–∏–ø–∞ –±–∏–∑–Ω–µ—Å–æ–≤: ◊¢◊ï◊°◊ß ◊§◊ò◊ï◊® (exempt business), ◊¢◊ï◊°◊ß ◊û◊ï◊®◊©◊î (registered business), ◊ó◊ë◊®◊ï◊™ ◊ë◊¢◊¥◊û (Israeli LLC)."
        },
        {
            "instruction": "–û—Ç–∫—É–¥–∞ –±–µ—Ä—É—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏?",
            "output": "–î–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∏–∑ 3 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: 1) PostgreSQL –∫—ç—à (datasets —Å data.gov.il - 716K –∫–æ–º–ø–∞–Ω–∏–π), 2) Real-time scraping (ica.justice.gov.il, court.gov.il), 3) Mock data –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (lib/checkid.ts)."
        },
        {
            "instruction": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏?",
            "output": "–°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Google Gemini 2.0 Flash –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Hebrew trust reports. Gemini –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç unified data (companies_registry, legal_cases, execution_proceedings) –∏ –≤—ã–¥–∞–µ—Ç trust score –æ—Ç 1.0 –¥–æ 5.0 –∑–≤–µ–∑–¥."
        }
    ]
    
    for faq in faq_data:
        dataset.append({
            "instruction": faq["instruction"],
            "input": "",
            "output": faq["output"],
            "system": "–¢—ã - AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–µ–∫—Ç–∞ TrustCheck Israel."
        })
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(dataset)}")
    return dataset

def save_dataset(dataset: List[Dict]):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–∞–π–ª"""
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ {OUTPUT_FILE}...")
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    file_size = OUTPUT_FILE.stat().st_size / 1024 / 1024
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_size:.2f} MB")
    print(f"üìä –ó–∞–ø–∏—Å–µ–π: {len(dataset)}")

def main():
    print("=" * 60)
    print("üöÄ TrustCheck AI Training Dataset Preparation")
    print("=" * 60)
    
    # 1. –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞
    files_data = collect_project_files()
    
    # 2. –ò–∑–≤–ª–µ—á—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
    url_data = extract_and_fetch_urls(files_data)
    
    # 3. –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    dataset = create_training_dataset(files_data, url_data)
    
    # 4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    save_dataset(dataset)
    
    print("\n" + "=" * 60)
    print("‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("=" * 60)
    print("\nüìã –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
    print("1. –ü—Ä–æ–≤–µ—Ä—å –¥–∞—Ç–∞—Å–µ—Ç: E:/LLaMA-Factory/data/trustcheck_knowledge_base.json")
    print("2. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π –≤ LLaMA Factory: data/dataset_info.json")
    print("3. –ó–∞–ø—É—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LLaMA Board")

if __name__ == "__main__":
    main()
